{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reads in all the books and pre-processes them to be ready to run different ML models on the data. It then exports this data to a csv to upload into other notebooks for said analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lrsterngmail.com/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lrsterngmail.com/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lrsterngmail.com/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import all need libaries \n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class that processes each book\n",
    "class Book():\n",
    "    \n",
    "    _all_books = []\n",
    "    \n",
    "    def __init__(self,filepath):\n",
    "        # assigns title and author\n",
    "        self.title = filepath.split('/')[1].rstrip('.txt')\n",
    "        self.author = filepath.split('/')[0]\n",
    "        \n",
    "    def read(self):\n",
    "        # opens the actual file\n",
    "        with open(filepath) as f:\n",
    "            self.text = f.readlines()\n",
    "            \n",
    "    def process(self):\n",
    "        # cleans book, takes out punctuation and formating \n",
    "        self.cleaned_book = []\n",
    "        for line in self.text:\n",
    "            for symbol in \"\"\"<>{}[]&*^%$#@~`_+=-\"\"'':;(),.?!''\\n\"\"\":\n",
    "                line = line.replace(symbol, '').lower()\n",
    "            self.cleaned_book.append(line)\n",
    "        \n",
    "    def tokenize(self):\n",
    "        # tokenize and remove stopwords\n",
    "        joined_book = ' '.join(self.cleaned_book)\n",
    "        tokenized_book = word_tokenize(joined_book)\n",
    "        stop_words=set(stopwords.words(\"english\"))\n",
    "        self.filtered_book = []\n",
    "        for w in tokenized_book:\n",
    "            if w not in stop_words:\n",
    "                self.filtered_book.append(w)\n",
    "        \n",
    "    def lemmatize(self):\n",
    "        # lemmatize the text\n",
    "        self.lemmatized_book=[]\n",
    "        for w in self.filtered_book:\n",
    "            self.lemmatized_book.append(lemmatizer.lemmatize(w))\n",
    "               \n",
    "    def add_book(self):\n",
    "        # adds each book to list of dictionaries of all books\n",
    "        Book._all_books.append({\n",
    "            'title':self.title,\n",
    "            'author':self.author,\n",
    "            'text':self.lemmatized_book,\n",
    "        })\n",
    "    \n",
    "    @classmethod\n",
    "    def all_books(cls):\n",
    "        return pd.DataFrame(cls._all_books)[['author','title','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of where all the individual books are saved as txt files\n",
    "list_of_file_paths = ['vep_shakespeare_tcp_v2_txt/AllsWellThatEndsWell.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads in all books and runs them thru the Book class\n",
    "for filepath in list_of_file_paths:\n",
    "    i = Book(filepath)\n",
    "    i.read()\n",
    "    i.process()\n",
    "    i.tokenize()\n",
    "    i.lemmatize()\n",
    "    i.add_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vep_shakespeare_tcp_v2_txt</td>\n",
       "      <td>AllsWellThatEndsWell</td>\n",
       "      <td>[delivering, son, bury, second, husband, going...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       author                 title  \\\n",
       "0  vep_shakespeare_tcp_v2_txt  AllsWellThatEndsWell   \n",
       "\n",
       "                                                text  \n",
       "0  [delivering, son, bury, second, husband, going...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to check all the data and make sure that it looks ok\n",
    "Book.all_books()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports all books to one csv for use in other notebooks for analysis\n",
    "Book.all_books().to_csv('all_books')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
